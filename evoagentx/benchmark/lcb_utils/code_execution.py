# copied from: https://github.com/LiveCodeBench/LiveCodeBench/blob/main/lcb_runner/benchmarks/code_execution.py

from datetime import datetime
from dataclasses import dataclass

from datasets import load_dataset


@dataclass
class CodeExecutionProblem:
    """Represents a code execution problem from the LiveCodeBench dataset.
    
    This class encapsulates all the information related to a code execution problem,
    including problem metadata, code, inputs, expected outputs, and evaluation methods.
    
    Attributes:
        question_id: Unique identifier for the question
        contest_id: Identifier for the programming contest
        contest_date: Date when the contest was held
        difficulty: Difficulty level of the problem
        function_name: Name of the function to implement
        code: Problem statement or code template
        input: Input test cases for the problem
        output: Expected output for the test cases
        id: Unique identifier for this problem instance
        problem_id: Identifier for the problem type
        numsteps: Number of evaluation steps/samples
    """
    question_id: str
    contest_id: str
    contest_date: datetime
    difficulty: str
    function_name: str
    code: str
    input: str
    output: str
    id: str
    problem_id: str
    numsteps: int

    def __post_init__(self):
        """Post-initialization processing.
        
        Called automatically after the dataclass initialization.
        Currently a placeholder for any future post-initialization logic.
        """
        pass

    def insert_output(self, output_list: list[str], pred_list: list[str]) -> dict:
        """Create a result dictionary with model outputs and predictions.
        
        Args:
            output_list: List of raw outputs from the model
            pred_list: List of processed predictions from the model
            
        Returns:
            Dictionary containing all problem data along with model outputs and predictions
        """
        return {
            "question_id": self.question_id,
            "contest_id": self.contest_id,
            "contest_date": self.contest_date.isoformat(),
            "difficulty": self.difficulty,
            "function_name": self.function_name,
            "code": self.code,
            "input": self.input,
            "output": self.output,
            "id": self.id,
            "problem_id": self.problem_id,
            "numsteps": self.numsteps,
            "output_list": output_list,
            "pred_list": pred_list,
        }

    def insert_output_evaluation(
        self, output_list: list[str], code_list: list[str], graded_list: list[bool]
    ) -> dict:
        """Create a result dictionary with model outputs and evaluation results.
        
        Extends the insert_output method by adding evaluation metrics, including
        the pass@1 score which represents the success rate of the model.
        
        Args:
            output_list: List of raw outputs from the model
            code_list: List of code solutions generated by the model
            graded_list: List of boolean values indicating whether each solution passed
            
        Returns:
            Dictionary containing all problem data, model outputs, and evaluation metrics
        """
        output = self.insert_output(output_list, code_list)
        output["graded_list"] = graded_list
        output["pass@1"] = graded_list.count(True) / len(graded_list)
        return output

    def get_evaluation_sample(self) -> dict:
        """Get a minimal sample for evaluation purposes.
        
        Returns a dictionary with just the essential fields needed for evaluating
        a solution: the problem code, input test cases, and expected output.
        
        Returns:
            Dictionary with code, input, and output fields
        """
        return {
            "code": self.code,
            "input": self.input,
            "output": self.output,
        }


def load_code_execution_dataset(release_version="release_v1", cache_dir: str = None) -> list[CodeExecutionProblem]:
    """Load the LiveCodeBench execution dataset.
    
    Retrieves the code execution problems from the LiveCodeBench dataset using the
    Hugging Face datasets library and converts them to CodeExecutionProblem objects.
    
    Args:
        release_version: Version of the dataset to load (default: "release_v1")
        cache_dir: Directory to cache the dataset (default: None, uses Hugging Face default)
        
    Returns:
        List of CodeExecutionProblem objects representing the dataset
    """
    dataset = load_dataset("livecodebench/execution-v2", split="test", trust_remote_code=True, cache_dir=cache_dir)  # type: ignore
    dataset = [CodeExecutionProblem(**p) for p in dataset]  # type: ignore
    # print(f"Loaded {len(dataset)} problems")
    return dataset


if __name__ == "__main__":
    dataset = load_code_execution_dataset()

# copied from: https://github.com/LiveCodeBench/LiveCodeBench/blob/main/lcb_runner/benchmarks/test_output_prediction.py

import json
from datetime import datetime
from dataclasses import dataclass
from datasets import load_dataset


@dataclass
class Test:
    """Represents a test case for the test output prediction task.
    
    A simple data structure that holds the input, expected output, and 
    test type information for a single test case.
    
    Attributes:
        input: The input data for the test case
        output: The expected output for the test case
        testtype: The type of test (e.g., "functional", "stdin")
    """
    input: str
    output: str
    testtype: str


@dataclass
class TestOutputPredictionProblem:
    """Represents a test output prediction problem from LiveCodeBench.
    
    This class encapsulates all the information related to a test output prediction
    problem, including problem metadata, test cases, and evaluation methods.
    
    Attributes:
        question_title: Title of the programming problem
        question_content: Detailed problem statement
        question_id: Unique identifier for the question
        contest_id: Identifier for the programming contest
        contest_date: Date when the contest was held
        difficulty: Problem difficulty level
        test: List of Test objects containing test cases
        starter_code: Initial code template provided to the user
        function_name: Name of the function to implement
        test_id: Identifier for the specific test
    """
    question_title: str
    question_content: str
    question_id: str
    contest_id: str
    contest_date: datetime
    difficulty: str
    test: list[Test]
    starter_code: str
    function_name: str
    test_id: int

    def __post_init__(self):
        """Post-initialization processing.
        
        Deserializes the test data from JSON to Test objects.
        """
        self.test = [Test(**t) for t in json.loads(self.test)]  # type: ignore

    def insert_output(self, output_list: list[str], pred_list: list[str]) -> dict:
        """Create a result dictionary with model outputs and predictions.
        
        Args:
            output_list: List of raw outputs from the model
            pred_list: List of processed predictions from the model
            
        Returns:
            Dictionary containing problem data along with model outputs and predictions
        """
        return {
            "question_title": self.question_title,
            "question_content": self.question_content,
            "question_id": self.question_id,
            "contest_id": self.contest_id,
            "contest_date": self.contest_date.isoformat(),
            "difficulty": self.difficulty,
            "output_list": output_list,
            "pred_list": pred_list,
            "test_id": self.test_id,
            "function_name": self.function_name,
            "starter_code": self.starter_code,
        }

    def insert_output_evaluation(
        self, output_list: list[str], code_list: list[str], graded_list: list[bool]
    ) -> dict:
        """Create a result dictionary with model outputs and evaluation results.
        
        Extends the insert_output method by adding evaluation metrics, including
        the pass@1 score which represents the success rate of the model.
        
        Args:
            output_list: List of raw outputs from the model
            code_list: List of code solutions or predictions generated by the model
            graded_list: List of boolean values indicating whether each solution passed
            
        Returns:
            Dictionary containing problem data, model outputs, and evaluation metrics
        """
        output = self.insert_output(output_list, code_list)
        output["graded_list"] = graded_list
        output["pass@1"] = graded_list.count(True) / len(graded_list)
        return output

    def get_evaluation_sample(self) -> dict:
        """Get a minimal sample for evaluation purposes.
        
        Returns a dictionary with just the essential fields needed for evaluating
        a solution: the problem statement and expected output.
        
        Returns:
            Dictionary with input (question content) and output (expected test output)
        """
        return {
            "input": self.question_content,
            "output": self.test[0].output,
        }


def load_test_prediction_dataset(release_version="release_v1", cache_dir: str = None,) -> list[TestOutputPredictionProblem]:
    """Load the LiveCodeBench test output prediction dataset.
    
    Retrieves the test output prediction problems from the LiveCodeBench dataset
    using the Hugging Face datasets library and converts them to 
    TestOutputPredictionProblem objects.
    
    Args:
        release_version: Version of the dataset to load (default: "release_v1")
        cache_dir: Directory to cache the dataset (default: None, uses Hugging Face default)
        
    Returns:
        List of TestOutputPredictionProblem objects representing the dataset
    """
    dataset = load_dataset("livecodebench/test_generation", split="test", trust_remote_code=True, cache_dir=cache_dir)  # type: ignore
    dataset = [TestOutputPredictionProblem(**d) for d in dataset]
    # print(f"Loaded {len(dataset)} prediction problems")
    return dataset


if __name__ == "__main__":
    dataset = load_test_prediction_dataset()
